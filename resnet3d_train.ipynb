{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ee9ac6",
   "metadata": {},
   "source": [
    "### Resnet3D Tanpa Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4542ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "[INFO] Loaded 1865 clips from /run/media/han/HDD RAIHAN/Real Dataset Cheating Comvis/Preprocessing/dataset_clips_split_ready/train\n",
      "[INFO] Loaded 392 clips from /run/media/han/HDD RAIHAN/Real Dataset Cheating Comvis/Preprocessing/dataset_clips_split_ready/val\n",
      "Initializing Pure ResNet3D-18 (Random Weights)...\n",
      "üöÄ Mulai Training...\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 234/234 [04:33<00:00,  1.17s/it]\n",
      "Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:13<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5459 | Acc: 0.7351\n",
      "Val   Loss: 0.4591 | Acc: 0.8112\n",
      "   -> Val loss decreased (inf --> 0.459139). Saving...\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  35%|‚ñà‚ñà‚ñà‚ñå      | 83/234 [01:40<03:02,  1.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 165\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     tl, ta = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     vl, va = val_epoch()\n\u001b[32m    168\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(tl); history[\u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m].append(ta)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    134\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m    135\u001b[39m scaler.scale(loss).backward()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m scaler.update()\n\u001b[32m    138\u001b[39m total_loss += loss.item() * clips.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Kuliah/S5/Comvis/venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:462\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m, (\n\u001b[32m    459\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Kuliah/S5/Comvis/venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:356\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    350\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m     **kwargs: Any,\n\u001b[32m    354\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    355\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf_per_device\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    357\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Kuliah/S5/Comvis/venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:356\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    350\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    353\u001b[39m     **kwargs: Any,\n\u001b[32m    354\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    355\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    357\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.video import r3d_18\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "DATA_ROOT = \"/run/media/han/HDD RAIHAN/Real Dataset Cheating Comvis/Preprocessing/dataset_clips_split_ready\"\n",
    "SAVE_PATH = \"/home/han/Documents/Kuliah/S5/Comvis/Projek/r3d_18_cheating_best_without_TL.pth\"\n",
    "CLIP_LEN = 32\n",
    "RESIZE = 112\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.0001\n",
    "EPOCHS = 100\n",
    "PATIENCE = 7\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- 1. DATASET CLASS ---\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, root, clip_len=32, resize=112):\n",
    "        self.root = root\n",
    "        self.clip_len = clip_len\n",
    "        self.resize = resize\n",
    "        self.samples = []\n",
    "        \n",
    "        if os.path.exists(root):\n",
    "            self.classes = sorted(os.listdir(root))\n",
    "            self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "            for cls in self.classes:\n",
    "                cls_dir = os.path.join(root, cls)\n",
    "                if not os.path.isdir(cls_dir): continue\n",
    "                for fname in os.listdir(cls_dir):\n",
    "                    if fname.endswith(\".mp4\"):\n",
    "                        self.samples.append((os.path.join(cls_dir, fname), self.class_to_idx[cls]))\n",
    "            print(f\"[INFO] Loaded {len(self.samples)} clips from {root}\")\n",
    "        else:\n",
    "            print(f\"[ERROR] Root not found: {root}\")\n",
    "            self.classes = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _sample_frames(self, cap, total_frames):\n",
    "        frames = []\n",
    "        needed = self.clip_len\n",
    "        if total_frames <= 0: return torch.zeros((3, needed, self.resize, self.resize), dtype=torch.float32)\n",
    "        \n",
    "        start = random.randint(0, total_frames - needed) if total_frames > needed else 0\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "        \n",
    "        count = 0\n",
    "        while count < needed:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: # Padding jika video habis/rusak\n",
    "                frames.append(np.zeros((self.resize, self.resize, 3), dtype=np.uint8))\n",
    "                count += 1\n",
    "                continue\n",
    "            try:\n",
    "                frame = cv2.resize(frame, (self.resize, self.resize))\n",
    "                frames.append(frame[:, :, ::-1]) # BGR to RGB\n",
    "                count += 1\n",
    "            except: continue\n",
    "            \n",
    "        frames = np.stack(frames).transpose(3, 0, 1, 2) / 255.0\n",
    "        return torch.tensor(frames, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        clip = self._sample_frames(cap, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "        cap.release()\n",
    "        return clip, torch.tensor(label)\n",
    "\n",
    "# --- 2. SETUP ---\n",
    "train_set = VideoClipDataset(os.path.join(DATA_ROOT, \"train\"), CLIP_LEN, RESIZE)\n",
    "val_set = VideoClipDataset(os.path.join(DATA_ROOT, \"val\"), CLIP_LEN, RESIZE)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- 3. MODEL (PURE RESNET / NO TRANSFER LEARNING) ---\n",
    "print(\"Initializing Pure ResNet3D-18 (Random Weights)...\")\n",
    "model = r3d_18(weights=None) # <--- MURNI (DARI NOL)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# --- 4. TRAINING HELPERS ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path='checkpoint.pth'):\n",
    "        self.patience = patience\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'   -> EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience: self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        print(f'   -> Val loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for clips, labels in tqdm(train_loader, desc=\"Train\"):\n",
    "        clips, labels = clips.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item() * clips.size(0)\n",
    "        _, pred = outputs.max(1)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "def val_epoch():\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in tqdm(val_loader, desc=\"Val\"):\n",
    "            clips, labels = clips.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(clips)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * clips.size(0)\n",
    "            _, pred = outputs.max(1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "early_stopping = EarlyStopping(patience=PATIENCE, path=SAVE_PATH)\n",
    "\n",
    "print(\"üöÄ Mulai Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    tl, ta = train_epoch()\n",
    "    vl, va = val_epoch()\n",
    "    \n",
    "    history['train_loss'].append(tl); history['train_acc'].append(ta)\n",
    "    history['val_loss'].append(vl); history['val_acc'].append(va)\n",
    "    \n",
    "    print(f\"Train Loss: {tl:.4f} | Acc: {ta:.4f}\")\n",
    "    print(f\"Val   Loss: {vl:.4f} | Acc: {va:.4f}\")\n",
    "    \n",
    "    early_stopping(vl, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"üõë Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(f\"‚úÖ Selesai. Model disimpan di: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd315f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pastikan cell training sudah selesai dijalankan agar variabel 'history' ada isinya.\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Grafik Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "plt.plot(history['val_loss'], label='Val Loss', color='red')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Grafik Akurasi\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc', color='blue')\n",
    "plt.plot(history['val_acc'], label='Val Acc', color='green')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6631e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r3d_18\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- KONFIGURASI TEST ---\n",
    "# Pastikan path ini sama dengan yang di Cell 1\n",
    "DATA_ROOT = \"/run/media/han/HDD RAIHAN/Real Dataset Cheating Comvis/Preprocessing/dataset_clips_split_ready\"\n",
    "MODEL_PATH = \"/home/han/Documents/Kuliah/S5/Comvis/Projek/r3d_18_cheating_best_without_TL.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Setup Dataset Test (Menggunakan Class dari Cell 1)\n",
    "test_set = VideoClipDataset(os.path.join(DATA_ROOT, \"test\"), clip_len=32, resize=112)\n",
    "test_loader = DataLoader(test_set, batch_size=8, shuffle=False, num_workers=2)\n",
    "class_names = test_set.classes\n",
    "\n",
    "print(f\"Testing pada {len(test_set)} klip video...\")\n",
    "\n",
    "if len(test_set) > 0:\n",
    "    # 1. INISIALISASI MODEL (STRUKTUR HARUS SAMA PERSIS DENGAN TRAIN)\n",
    "    # Karena train pakai Pure Resnet (weights=None), di sini juga harus sama.\n",
    "    model_test = r3d_18(weights=None) \n",
    "    model_test.fc = nn.Linear(model_test.fc.in_features, 2)\n",
    "    \n",
    "    # 2. LOAD WEIGHTS\n",
    "    print(f\"üì• Memuat model dari: {MODEL_PATH}\")\n",
    "    checkpoint = torch.load(MODEL_PATH)\n",
    "    model_test.load_state_dict(checkpoint)\n",
    "    model_test.to(DEVICE)\n",
    "    model_test.eval()\n",
    "\n",
    "    # 3. PREDIKSI\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"üîÑ Menjalankan prediksi...\")\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in tqdm(test_loader):\n",
    "            clips = clips.to(DEVICE)\n",
    "            outputs = model_test(clips)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    # 4. LAPORAN HASIL\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # 5. CONFUSION MATRIX\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Prediksi')\n",
    "    plt.ylabel('Asli')\n",
    "    plt.title('Confusion Matrix (Test Data)')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Folder test kosong atau tidak ditemukan!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
